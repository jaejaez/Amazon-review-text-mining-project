---
Course & Title: "APAN 5205 Data Cleaning"
Group: "Purrfect"
Members: "Drashti Shah, Cenrara Widi, Jaejae Zhang"

---
# This file contains data cleaning code for two raw datasets and the combined dataset

# DATASET 1: ELECTRONICS DATASET RAW

##STEP 1 Data Import

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Setting up the libraries
```{r library, include=FALSE}

library(jsonlite);library(dplyr);library(tidyr);library(ggplot2);library(lubridate);library(mice);library(stringr);library(tinytex);library(markdown)
```

#Setting the working directory and increasing the RAM allocation

```{r memory, echo=FALSE}
setwd("C:/Users/patel/Documents/Drashti Columbia Assignments")
memory.limit()
memory.limit(size=75000000)

```


#Importing the raw json dataset into a data frame
```{r import, echo=FALSE}

#Loading 20 million rows
electronicsdata=stream_in(file("Electronics.json"))
head(electronicsdata)
```

##STEP 2- Data Cleaning
```{r distinct rows, echo=FALSE}
#Removing the duplicate rows by combination of reviewerID and asin

electronicsdata=electronicsdata%>%
  distinct(asin,reviewerID,.keep_all = TRUE)
dim(electronicsdata)

#This reduced over 400,000 rows
 
```

```{r glimpse style, echo=FALSE}
#Now checking the data in column -style

glimpse(electronicsdata$style)

#Since, out of all 44 columns in that data frame, only format has values, we will drop all other columns within that data frame

```


```{r unpack and filter, echo=FALSE}
#Unpacking style data frame first


electronicsdata=electronicsdata%>%
  unpack(style)

names(electronicsdata)

#Checking all unique formats to see which ones to eliminate
unique(electronicsdata$`Format:`)

#Examining a few ambiguous formats
electronicsdata%>%
  filter(`Format:`==" Misc.")#Eliminating these as they are mostly book lights

electronicsdata%>%
  filter(`Format:`==" Misc. Supplies") #Eliminating these as well


#Eliminating all non-electronic formats

irrelevant_formats=c(" Hardcover"," Paperback"," Kindle Edition"," Library Binding"," Spiral-bound"," Mass Market Paperback",
                                               " Audible Audiobook"," Misc."," Misc. Supplies",
                          " Hardcover-spiral"," Plastic Comb"," Staple Bound"," Pamphlet"," Amazon Video" ," Tools & Home Improvement",
                          " Toy"," Baby Product"," Apparel" ," Unknown Binding"," Kitchen"," Health and Beauty"," Diary"," Map",
                          " Single Issue Magazine"," Comic"," Cards"," Grocery"," Calendar"," Box"," Loose Leaf"," Leather Bound"," Sports"," Perfect Paperback"," Vinyl Bound",
                     " Game"," Blu-ray"," Video Game"," CD-ROM"," DVD-ROM"," VHS Tape"," UMD for PSP"," Audio Cassette"," MP3 Music"," Software Download"," Software",
                     " Office Product"," Vinyl"," DVD"," Audio CD")
data=electronicsdata%>%
  filter(!`Format:` %in% irrelevant_formats)


#Checking unique formats in format column now-kept only electronics related formats
unique(data$`Format:`)

#There is NA in format but we will deal with it later when we join this dataset with metadata
data[is.na(data$`Format:`),]

```

```{r drop, echo=FALSE}
#Dropping irrelevant columns
names(data)
data=subset(data, select = -c(6:50,52,55))

```


```{r date formats, echo=FALSE}
#Changing unix review time to yyyy-mm-dd format

data$unixReviewTime=as.Date(as.POSIXct(data$unixReviewTime,origin="1970-01-01"))

#Eliminating reviewTime column
data=subset(data,select=-c(reviewTime))
names(data)
```


```{r recent data, echo=TRUE}
#Keeping only 2016-2018 data as Amazon changed its terms and conditions in October, 2016 
data=data %>%
 filter(unixReviewTime >= "2016-01-01")
#This narrows down the dataset to 9 million rows
dim(data)
```

```{r missing imputation, echo=FALSE}
# Handling NA's and Null's

# check NA's
sum(is.na(data$verified)) #Has 0 NA
sum(is.na(data$reviewText)) #Has 6876 NA's
sum(is.na(data$reviewerID))#Has 0 NA
sum(is.na(data$asin))#Has 0 NA
sum(is.na(data$vote))#Has over 8 million NA's
sum(is.na(data$overall))#Has 0 NA
sum(is.na(data$unixReviewTime))#Has 0 NA

dim(data)

#Dropping the rows where reviewText is NA
data=subset(data, !is.na(reviewText))

#Dropping vote column as more than 80% of the data is missing
data=subset(data,select=-c(vote))


```

```{r rename, echo=FALSE}
#Renaming a few columns
data=data %>% 
  rename(
    reviewTime= unixReviewTime,
    reviewScore=overall
    )

```

##STEP 3- Cleaning reviewText of punctuations, tabs, blank spaces, special characters, and non-graphical characters.
```{r clean review text, echo=FALSE}
data = data %>%
  mutate(reviewText = gsub("[[:punct:]]", "", reviewText), #Remove punctuation
         reviewText = gsub("[ |\t]{2,}", "", reviewText), #Remove tab
         reviewText = gsub("^ ", "", reviewText), #Remove blank space in the beginning
         reviewText = gsub(" $", "", reviewText), #Remove blank space at the end
         reviewText = gsub("[^[:graph:]]", " ", reviewText), #Remove non-graphical characters
         reviewText = gsub("[^[:alnum:]]", " ", reviewText), #Remove special characters
         reviewText = gsub("[^a-zA-Z0-9]", " ", reviewText)) #Remove other special characters

data$reviewText[1:20]

```


##STEP 4- Writing new csv
```{r csv, echo=FALSE}
#Glimpse of clean data
glimpse(data)

write.csv(data, "Clean_ElectronicsData.csv",row.names = F)


```

# DATASET 2- METADATA FOR ELECTRONICS DATASET RAW

##STEP 1 Data Import

#Since the metadata file is huge, we split the file where each file only consists of 50,000 JSON lines. The splitting process generated 16 files. Therefore, we assigned the path for each file to a variable.

```{r echo=TRUE}
file1 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_aa"
file2 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_ab"
file3 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_ac"
file4 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_ad"
file5 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_ae"
file6 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_af"
file7 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_ag"
file8 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_ah"
file9 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_ai"
file10 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_aj"
file11 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_ak"
file12 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_al"
file13 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_am"
file14 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_an"
file15 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_ao"
file16 <- "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/Split Data/meta_ap"
```

#Load all the split metadata files into data frames.

```{r echo=TRUE}
library(jsonlite)
meta1 <- stream_in(file(file1))
meta2 <- stream_in(file(file2))
meta3 <- stream_in(file(file3))
meta4 <- stream_in(file(file4))
meta5 <- stream_in(file(file5))
meta6 <- stream_in(file(file6))
meta7 <- stream_in(file(file7))
meta8 <- stream_in(file(file8))
meta9 <- stream_in(file(file9))
meta10 <- stream_in(file(file10))
meta11 <- stream_in(file(file11))
meta12 <- stream_in(file(file12))
meta13 <- stream_in(file(file13))
meta14 <- stream_in(file(file14))
meta15 <- stream_in(file(file15))
meta16 <- stream_in(file(file16))
```

Drop columns that will not be needed for analysis and only include relevant columns such as ASIN, product title or name, and main product category.

```{r echo=TRUE}
meta1 <- meta1[, c("asin", "title", "main_cat")]
meta2 <- meta2[, c("asin", "title", "main_cat")]
meta3 <- meta3[, c("asin", "title", "main_cat")]
meta4 <- meta4[, c("asin", "title", "main_cat")]
meta5 <- meta5[, c("asin", "title", "main_cat")]
meta6 <- meta6[, c("asin", "title", "main_cat")]
meta7 <- meta7[, c("asin", "title", "main_cat")]
meta8 <- meta8[, c("asin", "title", "main_cat")]
meta9 <- meta9[, c("asin", "title", "main_cat")]
meta10 <- meta10[, c("asin", "title", "main_cat")]
meta11 <- meta11[, c("asin", "title", "main_cat")]
meta12 <- meta12[, c("asin", "title", "main_cat")]
meta13 <- meta13[, c("asin", "title", "main_cat")]
meta14 <- meta14[, c("asin", "title", "main_cat")]
meta15 <- meta15[, c("asin", "title", "main_cat")]
meta16 <- meta16[, c("asin", "title", "main_cat")]
```

Combine all data frames into one

```{r echo=TRUE}
meta_data <- rbind(meta1, meta2, meta3, meta4, meta5, meta6, meta7, meta8, meta9, meta10, meta11, meta12, meta13, meta14,
                   meta15, meta16)
```

##STEP 2- Data Cleaning

#Remove duplicates of Product ID-asin column

```{r echo=TRUE}
library(dplyr)
meta_data <- meta_data %>%
  distinct(asin, .keep_all = TRUE)
```

#Clean up data by removing punctuations, tabs, blank spaces, special characters, and non-graphical characters.

```{r echo=TRUE}
library(stringr)
meta_data <- meta_data %>%
  mutate(title = gsub("[[:punct:]]", "", title), #Remove punctuation
         title = gsub("[ |\t]{2,}", "", title), #Remove tab
         title = gsub("^ ", "", title), #Remove blank space in the beginning
         title = gsub(" $", "", title), #Remove blank space at the end
         title = gsub("[^[:graph:]]", " ", title), #Remove non-graphical characters
         title = gsub("[^[:alnum:]]", " ", title), #Remove special characters
         title = gsub("[^a-zA-Z0-9]", " ", title), #Remove other special characters
         title = gsub("amp", "and", title)) %>% #Replace amp with and
  mutate(main_cat = gsub("[[:punct:]]", "", main_cat),
         main_cat = gsub("[ |\t]{2,}", "", main_cat),
         main_cat = gsub("^ ", "", main_cat), 
         main_cat = gsub(" $", "", main_cat),
         main_cat = gsub("[^[:graph:]]", " ", main_cat),
         main_cat = gsub("[^[:alnum:]]", " ", main_cat),
         main_cat = gsub("[^a-zA-Z0-9]", " ", main_cat),
         main_cat = gsub("amp", "and", main_cat))
```

#More clean up specifically for the main_cat column to replace empty values with NAs and combine repetitive category values.

```{r echo=TRUE}
meta_data <- meta_data %>%
  mutate(main_cat = gsub("Arts CraftsSewing", "Arts Crafts and Sewing", main_cat),
         main_cat = gsub("CameraPhoto", "Camera and Photo", main_cat),
         main_cat = gsub("Cell PhonesAccessories", "Cell Phones and Accessories", main_cat),
         main_cat = gsub("GPSNavigation", "GPS and Navigation", main_cat),
         main_cat = gsub("HealthPersonal Care", "Health and Personal Care", main_cat),
         main_cat = gsub("Home AudioTheater", "Home Audio and Theater", main_cat),
         main_cat = gsub("IndustrialScientific", "Industrial and Scientific", main_cat),
         main_cat = gsub("MoviesTV", "Movies and TV", main_cat),
         main_cat = gsub("Portable AudioAccessories", "Portable Audio and Accessories", main_cat),
         main_cat = gsub("SportsOutdoors", "Sports and Outdoors", main_cat),
         main_cat = gsub("ToolsHome Improvement", "Tools and Home Improvement", main_cat),
         main_cat = gsub("ToysGames", "Toys and Games", main_cat),
         main_cat = gsub(".*srchttps", NA, main_cat)) #Replace text pattern with NA

meta_data$main_cat[meta_data$main_cat == ""] <- NA #Replace empty category with NA
```

##STEP 3- Write the clean data into a csv file.

```{r echo=TRUE}
write.csv(meta_data, "/Users/cenrawidi/Documents/Columbia University/Classes/2021 Spring/APAN 5205/Group Project/meta_electronics.csv", row.names = FALSE)
```


# DATASET 3 JOININF ABOVE TWO DATASETS AND CLEANING

##STEP 1: Initial setup
```{r setup, include=FALSE}
library(ISLR); library(ggplot2); library(caret); library(tidyr); library(dplyr);
library(leaps); library(car); library(mice);library(tidyr);
library(data.table); library(ngram); library(stringr); library(corrplot);

memory.limit(75000000)
```


```{r data import}
setwd("C:/Users/patel/Documents/Drashti Columbia Assignments/")
data=read.csv("Clean_ElectronicsData.csv",na.strings = c("NA", "N/A", ""))
metadata=read.csv("Clean_metadata.csv",na.strings = c("NA", "N/A", ""))

#Looking at a few observations
head(data)
head(metadata)

```


```{r join}
#Joining the two datasets by column name- asin i.e. productID
data=left_join(data,metadata,by='asin')
head(data)
str(data)
```
```{r drop}
#Dropping rows that are not from electronics category
data=data%>%
  filter(!main_cat %in% c('Books','Collectible Coins','Gift Cards',
                          'Grocery'))
# All others are relevant
```

##STEP 2 Data Preparation and Final cleaning of Combined data


```{r NA}
dim(data)
names(data)

#Checking NA values after join- noting NA values before removing them
sum(is.na(data$verified)) #Has 0 NA
sum(is.na(data$reviewText)) #Has 1832 NA's
sum(is.na(data$reviewerID))#Has 0 NA
sum(is.na(data$asin))#Has 0 NA
sum(is.na(data$title))#Has 1128 NA
sum(is.na(data$reviewTime))#Has 0 NA
sum(is.na(data$main_cat))#Has 54223 NA

# Let's explore what titles exist in NA main category
data%>%
  select(title,main_cat)%>%
  filter(is.na(main_cat))

#Mostly accessories for laptops, phones, etc

#Removing all NA rows- negligent in reviewText, title,0.006% in main_cat
data=na.omit(data)
```

```{r,drop irrelevant columns}
#Adding a new column ID-
data=data%>%
  mutate(id=seq.int(nrow(data)))


#Changing the name of irrelevant columns-asin
names(data)[names(data) == "asin"] = "productID"
colnames(data)
dim(data)

#Number of distinct users
sum(!duplicated(data$reviewerID)) 

#Number of distinct products
sum(!duplicated(data$productID)) 
```

##STEP 3: Writing combied final csv for further analysis
```{r transform}
write.csv(data, "Clean_Simple_Data.csv",row.names = F)

```